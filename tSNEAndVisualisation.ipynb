{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from models import MLP, CombinedModel, ResNet\n",
    "from datasets_loaders import TaijiDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "def load_model(model_path, params_path):\n",
    "    with open(params_path, \"r\") as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    resnet_feature_extractor = ResNet()\n",
    "    mocap_model = MLP(params[\"mlp_mocap\"][\"input_dim\"], params[\"mlp_mocap\"][\"h1\"], params[\"mlp_mocap\"][\"h2\"], params[\"mlp_mocap\"][\"output_dim\"])\n",
    "    footp_model = MLP(params[\"mlp_footp\"][\"input_dim\"], params[\"mlp_footp\"][\"h1\"], params[\"mlp_footp\"][\"h2\"], params[\"mlp_footp\"][\"output_dim\"])\n",
    "\n",
    "    combined_model = CombinedModel(resnet_feature_extractor, mocap_model, params[\"mlp_mocap\"][\"output_dim\"],\n",
    "                                   footp_model, params[\"mlp_footp\"][\"output_dim\"], params[\"combined_model\"][\"num_classes\"])\n",
    "    combined_model.load_state_dict(torch.load(model_path))\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract softmax inputs\n",
    "def extract_softmax_inputs(model, dataloader, device):\n",
    "    softmax_inputs = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image_input, mocap_input, footp_input, labels in dataloader:\n",
    "            image_input = image_input.to(device)\n",
    "            mocap_input = mocap_input.to(device)\n",
    "            footp_input = footp_input.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get softmax inputs\n",
    "            model.eval()\n",
    "            x = model.resnet_feature_extractor(image_input)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            if model.mlp_mocap is not None:\n",
    "                mocap_input = model.mlp_mocap(mocap_input)\n",
    "                x = torch.cat((x, mocap_input), dim=1)\n",
    "            if model.mlp_footp is not None:\n",
    "                footp_input = model.mlp_footp(footp_input)\n",
    "                x = torch.cat((x, footp_input), dim=1)\n",
    "\n",
    "            x = F.relu(model.bn1(model.fc1(x)))\n",
    "            x = F.relu(model.bn2(model.fc2(x)))\n",
    "            x = model.bn3(model.fc3(x))\n",
    "\n",
    "            softmax_inputs.append(x.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    softmax_inputs = np.vstack(softmax_inputs)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    return softmax_inputs, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the t-SNE plot\n",
    "def compute_tsne(softmax_inputs, n_components=2, perplexity=30, random_state=42):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity, random_state=random_state)\n",
    "    tsne_results = tsne.fit_transform(softmax_inputs)\n",
    "    return tsne_results\n",
    "\n",
    "# Visualize the t-SNE plot\n",
    "def visualize_tsne(tsne_results, true_labels,num_classes,save_folder):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=true_labels, cmap=plt.cm.get_cmap(\"jet\", num_classes), marker='o', s=50)\n",
    "    plt.colorbar(ticks=range(num_classes))\n",
    "    plt.clim(-0.5, num_classes - 0.5)\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.title(\"t-SNE Visualization of Softmax Inputs\")\n",
    "    plt.savefig(save_folder+\"tsne_visualization_train_test.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Main function to load the model, extract softmax inputs, compute t-SNE and visualize the t-SNE plot\n",
    "\n",
    "def main(model_path, params_path, train_loader, test_loader, device,save_folder):\n",
    "    combined_model = load_model(model_path, params_path)\n",
    "    combined_model = combined_model.to(device)\n",
    "\n",
    "    print(\"Extracting training logits from combined_model\")\n",
    "    train_softmax_inputs, train_labels = extract_softmax_inputs(combined_model, train_loader,device)\n",
    "    print(\"Extracting test logits from combined_model\")\n",
    "    test_softmax_inputs, test_labels = extract_softmax_inputs(combined_model, test_loader,device)\n",
    "    \n",
    "    print(\"t-SNE fit transform for training data\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    train_tsne_results = tsne.fit_transform(train_softmax_inputs)\n",
    "    print(\"t-SNE fit transform for test data\")\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    test_tsne_results = tsne.fit_transform(test_softmax_inputs)\n",
    "\n",
    "    print(\"Generating t-SNE plot for train\")\n",
    "    visualize_tsne(train_tsne_results, train_labels, 46,save_folder+\"train_\")\n",
    "    print(\"Generating t-SNE plot for test\")\n",
    "    visualize_tsne(test_tsne_results, test_labels, 46,save_folder+\"test_\")\n",
    "    # visualize_tsne(train_tsne_results, test_tsne_results, train_labels, test_labels)\n",
    "\n",
    "device_ = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_root_dir = './dataset_100_per_pose'\n",
    "batch_size = 32\n",
    "# Example usage\n",
    "for test_subj in range(1,11):\n",
    "    test_subject = 'Subject'+str(test_subj)\n",
    "    print(\"test_subject : \",test_subject)\n",
    "    results_root_dir = \"./runs/ten_subjs_run8_10_epochs/\" + test_subject\n",
    "    save_folder_ = results_root_dir+\"/\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    model_path = results_root_dir + \"/combined_model.pth\"\n",
    "    params_path = results_root_dir + \"/training_params.json\"\n",
    "\n",
    "    train_dataset = TaijiDataset(dataset_root_dir, transform=transform, test=False, test_subject=test_subject)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    test_dataset = TaijiDataset(dataset_root_dir, transform=transform, test=True, test_subject=test_subject)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    main(model_path, params_path, train_loader, test_loader,device_, save_folder_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/paperspace/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/paperspace/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/paperspace/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# results_root_dir = \"./runs/ten_subjs_run8_10_epochs/\" + test_subject\n",
    "model_path_  = 'runs/ten_subjs_run8_10_epochs/Subject1/combined_model.pth'\n",
    "params_path_ = 'runs/ten_subjs_run8_10_epochs/Subject1/training_params.json'\n",
    "trained_model = load_model(model_path_, params_path_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_maps(images, feature_maps, title):\n",
    "    num_classes = len(images)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        fig, axs = plt.subplots(3, 3, figsize=(9, 9))\n",
    "        \n",
    "        # Show input image\n",
    "        axs[0, 0].imshow(images[i].cpu().permute(1, 2, 0))\n",
    "        axs[0, 0].set_title(f\"Class {i} Image\", fontsize=10)\n",
    "        axs[0, 0].axis(\"off\")\n",
    "        \n",
    "        # Show first 8 feature maps\n",
    "        for j in range(1,9):\n",
    "            row = j // 3 #+ 1\n",
    "            col = j % 3\n",
    "            # print(\"----------\")\n",
    "            # print(\"row : \",row)\n",
    "            # print(\"col : \",col)\n",
    "            axs[row, col].imshow(feature_maps[i][j].detach().cpu(), cmap=\"viridis\")\n",
    "            axs[row, col].set_title(f\"Feature Map {j+1}\", fontsize=10)\n",
    "            axs[row, col].axis(\"off\")\n",
    "            \n",
    "        fig.suptitle(f\"{title} for Class {i}\", fontsize=14, y=0.93)\n",
    "        plt.savefig(f\"{title}_feature_maps_class_{i}.png\", bbox_inches=\"tight\")\n",
    "        plt.show()\n",
    "\n",
    "def visualize_resnet_feature_maps(model, layer_num, images, device):\n",
    "    # Get the selected layer from the model\n",
    "    layer = model.feature_extractor[layer_num]\n",
    "    images = [img.to(device) for img in images]\n",
    "\n",
    "    feature_maps = []\n",
    "    for img in images:\n",
    "        img = img.unsqueeze(0)  # Add batch dimension\n",
    "        x = img\n",
    "        for l in model.feature_extractor[:layer_num + 1]:\n",
    "            # print(\"l : \",l)\n",
    "            x = l(x)\n",
    "        feature_maps.append(x[0])\n",
    "\n",
    "    display_feature_maps(images, feature_maps, f\"ResNet Feature Maps Layer {layer_num}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "\n",
    "transform_ = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Provide one image per class to visualize their feature maps\n",
    "example_images = []\n",
    "test_subject = 'Subject1'\n",
    "print(\"test_subject : \",test_subject)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for pose in range(46):\n",
    "    temp_dir = \"./dataset_100_per_pose/Subject1/\"+str(pose)+\"/\"\n",
    "    img_path = glob.glob(temp_dir+\"*.jpg\")[0]\n",
    "    image = Image.open(img_path)\n",
    "    image = transform_(image).to(device)\n",
    "    example_images.append(image)\n",
    "\n",
    "\n",
    "# Select the layer number from which to extract the feature maps\n",
    "layer_num = 4  # Choose a layer number\n",
    "trained_model = trained_model.to(device)\n",
    "\n",
    "visualize_resnet_feature_maps(trained_model.resnet_feature_extractor, layer_num, example_images, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
